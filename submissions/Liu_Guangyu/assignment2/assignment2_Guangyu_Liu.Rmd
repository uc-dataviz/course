---
title: "Assignment2: experimenting with visual design"
author: "Guangyu Liu"
date: "Due: April 28, 2017"
output: github_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyr)
library(tidyverse)
library(knitr)
```

### Introduction
According to Cairo (2012), the brain loves differences, and it is able to detect size, orientation, and shade preattentively. All these features, plus color, which is common in denoting different groups, however, are at the bottom of Cleveland and McGill's (1984) accuracy list. Cleveland and McGill designed 10 elementary perceptual tasks to rank several common ways of representing data. Heer and Bostock (2010) replicated their research using crowdsourced experiments and got similiar results to the originals. Both studies focused on continuous variables. Cleveland and McGill's list had position and length, which allow more accurate judgments, on the top, while area and color, which allow more generic judgments (Cairo, 2012), at the bottom. However, color saturation, shading, shape, and size (area) are common channels used to represent categorical variables. Different from continuous variables, which require the audience to tell what percentage one value is of the other, what crucial to categorical variables is to easily differentiate one from the others.     
    
In this assignment, I plan to test how the brain is good at differentiating colors, shapes, shadings, and sizes using mTurk and Qualtrics. By good, I mean quickly and accurately detect the difference. The hypothesises are below:    
    
- The brain detects color, shading, shape, and size with different speeds.
- The brain detects color, shading, shape, and size with different accuracies.

### Methods
#### Research design
This experiment includes two parts. First, I tested how good the brain can find items represented by a certain feature. Second, I tested how good the brain can tell a general trend represented by a certain feature. Both tasks are frequently used in exploratory analysis when one wants to compare characteristics between groups.    
    
For the first task, I generated two data frames, each has 46 and 50 items respectively. `x` and `y` are random numbers, following standard normal distribution. `class` is a categorical variable with four categories. In each data frame, four categories have similiar number of points. All together, two dataframes have eight categories with number of points range from 11 to 13. I intentionly chose similiar but not identical number of items in each category, in order to reduce the probability of respondents guessing answer while also allow the time spent on different questions comparable.    
    
I mapped `x` to x-axis, `y` to y-axis, and `class` to one of the channels -- color, shading, size, and shape. For each channel, I rerandomized `x` and `y` in the two data frames and generated two scatter plots. The respondents are required to count the number of points within a category. An example of plotting a color channeled graph is as below. For this graphs, The respondents are asked to count how many points are 'A'.    
    
```{r}
set.seed(428)

# Generate a random N-item data frame, with x and y are normal distributed continuous variables,
# class is a categorical variable with 4 categories, while each category has nearly equal number of  items
N1 <- 46
rData_1 <- data.frame(x = abs(rnorm(N1)), y = abs(rnorm(N1)), class = c(1:N1) %% 4)
rData_1$class <- factor(as.factor(rData_1$class), labels = c("A", "B", "C", "D"))

# Scatter plot
ggplot(rData_1) +
  geom_point(aes(x = x, y = y, color = class), size = 4) +
  labs(x = "", y = "", title = "", color = "") +
  scale_x_continuous(breaks = NULL) +
  scale_y_continuous(breaks = NULL)
```

For the second task, I generated three scatter plots for each channel using a data frame of 50 items. In each plot, there is a category in which `y` is possitively associated with `x` (`y = 0.6x + 0.3y'`), or negatively associated with `x` (`y = -0.6x + 0.3y'`), or a constant (`y = 0.3y'`), where `x` and `y'` are both normally distributed random numbers. So the plot shows a general trend between `x` and `y` with variations, rather than a perfectly fitted line. `x` and `y` in other categories are unrelated random numbers. `x`, `y`, and `class` are defined as above. The respondents are required to tell when `x` increases, how will `y` change. An example of plotting a shape channeled, `x` and `y` positively associated graph is as below. For this graph, the respondents are asked to tell for class D, how will `y` change when `x` increases.    
```{r}
# General trend by shape
# Positive association
N <- 50
rData <- data.frame(x = rnorm(N), y = rnorm(N), class = c(1:N) %% 4)
rData$class <- factor(as.factor(rData$class), labels = c("A", "B", "C", "D"))
rData$y <- ifelse(rData$class == "D", 0.6*rData$x + 0.3*rData$y, rData$y)

ggplot(rData) +
  geom_point(aes(x = x, y = y, shape = class), size = 4) +
  labs(x = "x", y = "y", title = "") +
  scale_x_continuous(breaks = NULL) +
  scale_y_continuous(breaks = NULL)
```

#### Data collection
Eight questions for task 1, two for each channel, and twelve questions for task 2, three for each channel are created. I designed a [survey](http://ssd.az1.qualtrics.com/jfe/form/SV_eKCcpICSV9YLjIF) using Qualtrics and posted the link on MTurk to collect data. In a pilot study, I found respondents will answer questions more and more quickly as they get familiar with the question type. So questions appear at the beginning take longer time while questions in the end take shorter time. To minimize the order effect, I randomized all 20 questions. Both the respondents' answerand the time spent on each question are recorded. The original sample size is 21.    
    
Because it is difficult to control the experiment environment on MTurk, some creteria are applied to remove "guessing answers". For the first task (counting points), it is difficult to read the question, understand the legend, and count points within 5 seconds. Time spent less than 5 seconds is regarded as unreasonably short and was converted as missing value (N/A). Most respondents spent less than 30 seconds on each question. Time spent more than 60 seconds is regarded as unreasonably long and was converted as missing value. For the second task (multiple choice), two respondents chose the same option for all twelve questions, and spent relatively short time on each quesion. Their answers are regarded as "guessing answer", and all their answers are removed from the dataset. After that, the valid sample size is 19.

### Results
#### Counting points
For the first task, respondents are required to count the number of points belong to a category. I recoded the answer into the deviation from the correct number. Take the first graph in "Research Design" section as an example. Respondents are asked to count how many points are "A". The correct number is 11. If one answers 13, he/she has 2 points deviation from the correct number. If one answers 10, he/she has -1 point deviation from the correct number. Then I calculated the mean, min, max, and sd of the deviation of each channel. Channels with a mean closer to zero can convey categorical information more accurately.    
    
Similarly, I transformed the time spent on each question. Since there is no "correct time", I calculated the time deviation from the grand mean, i.e. the average time of all eight questions in task 1. Then, the mean, min, max, and sd of time deviation of each channel are calculated. If a channel has a negative time deviation, people can differentiate points represented by this channel quicker than average. If a channel has a positive time deviation, it will take longer time for the brain to differentiate points represented by this channel.    
    
As an exploratory analysis, I graphed the answer deviation from the correct number and the time deviation from the grand mean in `Fig 1`. The points denote the means of each channel, the solid lines are mean +/- sd, and the dotted lines are the ranges of the deviation of each channel.    
```{r}
mTurkData <- read.csv("cleanedData.csv")

# Task1 -- counting points
task1 <- select(mTurkData, num_range("q", 1:8)) %>% 
  gather(key = "channel", value = "answer") %>% 
  mutate(channel = ifelse(channel == "q1" | channel == "q8", "color",
                          ifelse(channel == "q2" | channel == "q7", "shading",
                                 ifelse(channel == "q3" | channel == "q6", "size", "shape"))))

time1 <- select(mTurkData, num_range("t", 1:8)) %>% 
  gather(key = "channel", value = "timeSpent") %>% 
  mutate(channel = ifelse(channel == "t1" | channel == "t8", "color",
                          ifelse(channel == "t2" | channel == "t7", "shading",
                                 ifelse(channel == "t3" | channel == "t6", "size", "shape"))),
         centeredT = timeSpent - mean(timeSpent, na.rm = TRUE))

# Calculate the mean, min, max, and sd of the deviation
task1_sum <- task1 %>% 
  group_by(channel) %>% 
  summarize(mean = mean(answer, na.rm = TRUE), 
            min = min(answer, na.rm = TRUE), 
            max = max(answer, na.rm = TRUE), 
            sd = sd(answer, na.rm = TRUE))

time1_sum <- time1 %>% 
  group_by(channel) %>% 
  summarize(mean = mean(centeredT, na.rm = TRUE), 
            min = min(centeredT, na.rm = TRUE), 
            max = max(centeredT, na.rm = TRUE), 
            sd = sd(centeredT, na.rm = TRUE))

# Merge data and reorder factors
task1_merge <- bind_rows("Answer" = task1_sum, "Time" = time1_sum, .id = "group")
task1_merge$channel <- reorder(as.factor(task1_merge$channel), task1_merge$mean)
task1_merge$group <- factor(task1_merge$group, 
                            levels = c("Time", "Answer"), 
                            labels = c("Time", "Answer"))


# Plot the answer and time of each channel. 
# The point in the middle denotes the mean of each channel. Answer close to zero means more accurate.
# The dotted line denotes the range of the deviation from the correct answer/grand mean.
# The solid line denotes [mean - sd, mean + sd]
ggplot(task1_merge, aes(x = group, y = mean, color = group)) +
  geom_linerange(aes(ymin = min, ymax = max), size = 1, linetype = 3) +
  geom_linerange(aes(ymin = mean - sd, ymax = mean + sd), size = 1) +
  geom_point(size = 2) +
  geom_hline(yintercept = 0, color = "ivory3", linetype = 2, size = 0.8) +
  coord_flip() +
  facet_grid(channel~.) +
  labs(y = "Deviation",
       x = NULL,
       title = "Fig 1 - Differentiating points represented by different channels") +
  scale_color_manual(values = c("Time" = "skyblue1", "Answer" = "coral1")) +
  theme(legend.position = "none")
```
    
The graph above shows that from color to shape to shading to size, the accuracy in conveying categorical variables decreases while time used increases. To see if the differences are statistically significant, I ran t-test for every two adjacent channels.
```{r}
# T-test for every adjacent channels
# Define a function to add t-test results to a dataframe
add_result <- function(df, result, test){
  df <- add_row(df, test = test, t_stat = result$statistic, p_value = result$p.value)
}

# New dataframe
ttest1 <- data.frame(test = character(0), t_stat = numeric(0), p_value = numeric(0))

# T-tests and adding results
result <- t.test(answer ~ channel, data = filter(task1, channel == "color" | channel == "shape"))
ttest1 <- add_result(ttest1, result, "Answer: color~shape")

result <- t.test(answer ~ channel, data = filter(task1, channel == "shape" | channel == "shading"))
ttest1 <- add_result(ttest1, result, "Answer: shape~shading")

result <- t.test(answer ~ channel, data = filter(task1, channel == "shading" | channel == "size"))
ttest1 <- add_result(ttest1, result, "Answer: shading~size")

result <- t.test(centeredT ~ channel, data = filter(time1, channel == "color" | channel == "shape"))
ttest1 <- add_result(ttest1, result, "Time: color~shape")

result <- t.test(centeredT ~ channel, data = filter(time1, channel == "shape" | channel == "shading"))
ttest1 <- add_result(ttest1, result, "Time: shape~shading")

result <- t.test(centeredT ~ channel, data = filter(time1, channel == "shading" | channel == "size"))
ttest1 <- add_result(ttest1, result, "Time: shading~size")

kable(ttest1, format = "markdown", col.names = c("Test", "t-stat", "p-value"),
      caption = "T-test output")
```
The statistical analysis shows that `color` has the highest accuracy, while `size` has the lowest accuracy. `shape` and `shading` are in the middle and are not statistically different from each other. Regarding speed, our brain can differentiate `color` and `shape` statistically faster than `shading` and `size`.    
    
#### General trend
As above, I recoded answers and time spent on task 2 as deviation from the correct answer and the grand mean. Since task 2 only has multiple choice questions, and the answers are either true (0 unit deviates from the correct answer) or false (1 unit deviates from the correct answer), it is meaningless to plot the range of the answer. In the meanwhile, time spent on task 2 varies a lot. Including the min and max time deviation will shrink the difference among mean time deviations shown in the plot. Therefore, only means and standard deviations are plotted in `Fig 2`.
```{r}
task2 <- select(mTurkData, num_range("q", 9:20)) %>% 
  gather(key = "channel", value = "answer") %>% 
  mutate(channel = ifelse(channel == "q9" | channel == "q10" | channel == "q11", "color",
                          ifelse(channel == "q12" | channel == "q13" | channel == "q14", "shape",
                                 ifelse(channel == "q15" | channel == "q16" | channel == "q17", "size", "shading"))))

# Now, correct answer is coded as TRUE (1), wrong answer is coded as FALSE (0)
# Recode correct answer as 0 point deviates from the correct answer, and wrong answer as 1 point deviates from the correct answer
# so that the graph can be read the same as task 1
task2$answer <- 1 - task2$answer

time2 <- select(mTurkData, num_range("t", 9:20)) %>% 
  gather(key = "channel", value = "timeSpent") %>% 
  mutate(channel = ifelse(channel == "t9" | channel == "t10" | channel == "t11", "color",
                          ifelse(channel == "t12" | channel == "t13" | channel == "t14", "shape",
                                 ifelse(channel == "t15" | channel == "t16" | channel == "t17", "size", "shading"))),
         centeredT = timeSpent - mean(timeSpent, na.rm = TRUE))

# Calculate the mean, min, max, and sd of the deviation
task2_sum <- task2 %>% 
  group_by(channel) %>% 
  summarize(mean = mean(answer, na.rm = TRUE), 
            sd = sd(answer, na.rm = TRUE))

time2_sum <- time2 %>% 
  group_by(channel) %>% 
  summarize(mean = mean(centeredT, na.rm = TRUE), 
            sd = sd(centeredT, na.rm = TRUE))

# Merge data and reorder factors
task2_merge <- bind_rows("Answer" = task2_sum, "Time" = time2_sum, .id = "group")
task2_merge$channel <- reorder(as.factor(task2_merge$channel), task2_merge$mean)
task2_merge$group <- factor(task2_merge$group, 
                            levels = c("Time", "Answer"), 
                            labels = c("Time", "Answer"))


# Plot the answer and time of each channel. 
# The point in the middle denotes the mean of each channel. Answer close to zero means more accurate.
# The dotted line denotes the range of the deviation from the correct answer/grand mean.
# The solid line denotes [mean - sd, mean + sd]
ggplot(task2_merge, aes(x = group, y = mean, color = group)) +
  geom_linerange(aes(ymin = mean - sd, ymax = mean + sd), size = 1) +
  geom_point(size = 2) +
  geom_hline(yintercept = 0, color = "ivory3", linetype = 2, size = 0.8) +
  coord_flip() +
  facet_grid(channel~.) +
  labs(y = "Deviation",
       x = NULL,
       title = "Fig 2 - Telling general trends represented by different channels") +
  scale_color_manual(values = c("Time" = "skyblue1", "Answer" = "coral1")) +
  theme(legend.position = "none")
```
    
`Fig 2` shows that the brain is able to tell the general trend shown by `color` faster than average, and it needs more time to tell the general trends shown by `shading` and `size`. The difference in accuracy rate, however, is difficult to tell from the graph. Again, I ran statistical test to see if there is significant difference among channels.
```{r}
# T-test for every adjacent channels
# Define a function to add t-test results to a dataframe
add_result <- function(df, result, test){
  df <- add_row(df, test = test, stat = result$statistic, p_value = result$p.value)
}

# New dataframe
ttest2 <- data.frame(test = character(0), stat = numeric(0), p_value = numeric(0))

# T-tests and adding results
result <- oneway.test(answer ~ channel, data = task2)
ttest2 <- add_result(ttest2, result, "Answer: one-way ANOVA")

result <- oneway.test(centeredT ~ channel, data = time2)
ttest2 <- add_result(ttest2, result, "Time: one-way ANOVA")

kable(ttest2, format = "markdown", col.names = c("Test", "F-stat", "p-value"),
      caption = "One-way ANOVA output")
```
Statistical analysis shows that there is no difference among channels in accurately telling the trend. Although it is easier to tell a trend represented by color than shape than shading than size, the differences are not statistically significant.    
    
### Conclusion and Discussion
According to the results above, our brain can differentiate points represented by different colors best and by different sizes worst, in terms of accuracy and speed. But when there is a trend in the plot, the difference diminishes. This suggests to use color rather than size when plotting the distribution of different categories, while these four channels work similar good in representing trends.    
    
However, in both tasks, there is a large variation in the time spent, even within the same respondent. It is difficult to control the experiment environment using MTurk. A respondent may randomly click an answer, which results very short time spent, or he/she may rest for several seconds before really starts to answer a question, which results longer time. The measurement error may bias the experiment results.  
    
**The full code for this assignment can be found in [assignment2.R](https://github.com/guangyu-liu/dataviz/blob/master/submissions/Liu_Guangyu/assignment2/assignment2.R)**    
    
### References    
Cairo, A. (2012). The Functional Art: An introduction to information graphics and visualization. New Riders. Retrieved from http://proquestcombo.safaribooksonline.com.proxy.uchicago.edu/book/graphic-design/9780133041187    
    
Cleveland, W. S., & McGill, R. (1984). Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods. Journal of the American Statistical Association, 79(387), 531–554. https://doi.org/10.2307/2288400    
    
Heer, J., & Bostock, M. (2010). Crowdsourcing Graphical Perception: Using Mechanical Turk to Assess Visualization Design. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (pp. 203–212). New York, NY, USA: ACM. https://doi.org/10.1145/1753326.1753357
